{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to the Black-Litterman in Python\n",
    "\n",
    "## Introduction\n",
    "### Background and Theory\n",
    "\n",
    "The Black-Litterman asset allocation model \\cite{black1992global}, \\cite{he1999intuition} provides a methodical way of combining an investors subjective views of the future performance of a risky investment asset with the views implied by the market equilibrium. The method has seen wide acceptance amongst practitioners as well as academics in spite of the fact that it originated as an internal Goldman Sachs working paper, rather than as a piece of research from academia.\n",
    "\n",
    "The Black Litterman procedure can be viewed as a bayesian shrinkage method, that shrinks the expected returns constructed from an investor's views on asset returns towards asset returns implied by the market equilibrium. The procedure computes a set of expected returns that uses the market equilibrium implied  as a prior. This is then combined with returns implied by subjective investor views to produce a set of posterior expected returns $\\mu^{BL}$ and covariances $\\Sigma^{BL}$.\n",
    "\n",
    "Besides the obvious attraction of being able to incorporate subjective investor views, the Black-Litterman procedure has a second feature that makes it extremely attractive to portfolio optimization. It is well known that the Markowitz optimization procedure is highly sensitive to estimation errors in Expected Returns and Covariances, and this _error maximizing_ nature of the Markowitz procedure causes unstable portfolios with extreme weights that diverge rapidly from the market equilibrium portfolio even with minor changes to the inputs (e.g. \\cite{chopra1993effect}, \\cite{michaud1989markowitz}). However, the posterior parameters $\\mu^{BL}, \\Sigma^{BL}$ computed by the Black Litterman procedure are derived in part from the market portfolio, and therefore are much more pragmatic inputs for purposes of portfolio optimization. Specifically, when $\\mu^{BL}, \\Sigma^{BL}$ as used as as inputs to a Markowitz Optimizer, they produce optimized weights that diverge from the market portfolio in limited ways, and only to the extent of the confidence that the investor expresses in the views. Consequently the optimized portfolios are more stable portfolios than with pure Markowitz optimization with sample estimates. In the extreme, with appropriately set parameters, the Markowitz portfolio computed from the Black-Litterman parameters when there are no subjective investor views exactly coincides and is able to recover the market equilibrium portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Black Litterman Formulas\n",
    "\n",
    "Assume that we have $N$ assets, and $K$ views. There are two sets of inputs to the procedure. The first set of inputs relate to market parameters and these are:\n",
    "\n",
    "\\begin{array}{ll}\n",
    "w & \\mbox{A Column Vector ($N \\times 1$) of Equilibrium Market Weights of the Assets} \\\\\n",
    "\\Sigma & \\mbox{A Covariance Matrix ($N \\times N$) of the Assets} \\\\\n",
    "R_f & \\mbox{The Risk Free Rate} \\\\\n",
    "\\delta & \\mbox{The investor's Risk Aversion parameter}  \\\\\n",
    "\\tau & \\mbox{A scalar indicating the uncertainty of the prior (details below)}\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "Some of these parameters can be inferred from other parameters if they are not explicitly specified. For instance, the risk aversion parameter can be set arbitrarily. For instance, some authors use $\\delta = 2.5$ while others use the value of $\\delta = 2.14$ in order to be consistent with the value calculated in \\cite{dimson2008triumph}.\n",
    "\n",
    "\\cite{beach2007application} suggest using $2.65$. Another common approach is to set $\\delta$ to the Market Price of Risk (i.e. a measure of the risk aversion of the Representative Investor, which is computed as $\\delta = \\mu_M/\\sigma^2_M$ where $\\mu_M$ and $\\sigma^2_M$ are estimates of the mean and variance of the returns of the market portfolio. Frequently, a broad market index such as the S\\&P500 is taken as a proxy for the market in order to compute the market price of risk from $\\mu_M$ and $\\sigma^2_M$.\n",
    "\n",
    "The treatment of $\\tau$ is the source of some confusion. As we will explain in the following section, some implementors have done away with $\\tau$ by setting it to $1$ or to calibrate the model to $tau$. In the original model, Black and Litterman suggest using a small number. A common technique is to set $\\tau = 1/T$ where $T$ is the number of periods of data used. Thus, for $T=5$ you would use $1/(5 \\times 12)$ which yields a value of approximately $\\tau=.02$.\n",
    "\n",
    "The second set of inputs that the procedure needs is a representation of the investors views. These are specified via:\n",
    "\n",
    "\\begin{array}{ll}\n",
    "Q & \\mbox{An $K \\times 1$ ``Qualitative Views'' or simply, Views matrix} \\\\\n",
    "P & \\mbox{A $K \\times N$ ``Projection'' or ``Pick'' matrix, linking each view to the assets} \\\\\n",
    "\\Omega & \\mbox{A Covariance matrix representing the uncertainty of views}\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "Views are represented in $Q$ and $P$ as follows:\n",
    "\n",
    "If the $k$-th view is an absolute view, it is represented by setting $Q_k$ to the expected return of asset $k$ and setting $P_{ki}$ to 1 and all other elements of row $k$ in $P$ to zero.\n",
    "\n",
    "If the $k$-th view is an relative view, between assets $i$ and $j$ it is represented by setting $Q_k$ to the expected difference of returns between assets $i$ and $j$, and setting $P_{ki}$ to $-1$ for the underperforming asset, $P_{kj}$ to $+1$ and all other elements of row $k$ in $P$ to zero. $\\Omega$ is either set to the specified uncertainty or is inferred from the user or from the data.\n",
    "\n",
    "The uncertainty of the views $\\Omega$ is either set by the user, or inferred (e.g. via statements of confidence, from market data, from the variance of residuals from a prediction model used to generate the views etc, we shall see examples in sections below). In particular, \\cite{he1999intuition} suggest setting it to be the diagonal matrix obtained from the diagonal elements of $P \\tau \\Sigma P^T$, which is what we shall do for some of our initial tests. In my implementation the code accepts a matrix, but uses this assumption as the default if the user does not specify a matrix to use as $\\Omega$.\n",
    "\n",
    "#### The Master Formula\n",
    "\n",
    "The first step of the procedure is a _reverse-optimization_ step that infers the implied returns vector $\\pi$ that are implied by the equilibrium weights $w$ using the formula:\n",
    "\n",
    "$$\\pi = \\delta\\Sigma w$$\n",
    "\n",
    "Next, the posterior returns and covariances are obtained from the _Black-Litterman Master Formula_ which is the following set of equations:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:blMuOrig}\n",
    "\\mu^{BL} = [(\\tau\\Sigma)^{-1} + P \\Omega^{-1} P]^{-1}[(\\tau\\Sigma)^{-1} \\pi + P \\Omega^{-1} Q]\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:blSigmaOrig}\n",
    "\\Sigma^{BL} = \\Sigma + [(\\tau\\Sigma)^{-1} + P \\Omega^{-1} P]^{-1}\n",
    "\\end{equation}\n",
    "\n",
    "#### Inverting $\\Omega$\n",
    "\n",
    "While the master formulas identified in Equation \\ref{eq:blMuOrig} and Equation \\ref{eq:blSigmaOrig} are frequently easy to implement, they do involve the term $\\Omega^{-1}$. Unfortuantely, $\\Omega$ is sometimes non-invertible, which poses difficulties to implement the equations as-is. Fortunately the equations are easily transformed to a form that does not require this troublesome inversion. Therefore, frequently, implementations use the following equivalent versions of these equations which are sometimes computationally more stable, since they do not involve inverting $\\Omega$. Derivations of these alternate forms are provided in the appendices of \\cite{walters2011black}:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:blMu}\n",
    "\\mu^{BL} = \\pi + \\tau \\Sigma P^T[(P \\tau \\Sigma P^T) + \\Omega]^{-1}[Q - P \\pi]\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:blSigma}\n",
    "\\Sigma^{BL} = \\Sigma + \\tau \\Sigma - \\tau\\Sigma P^T(P \\tau \\Sigma P^T + \\Omega)^{-1} P \\tau \\Sigma\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flavors of Black-Litterman\n",
    "\n",
    "The original method described above has also seen a number of modifications and extensions (e.g. see \\cite{walters2011black} for an extensive and detailed summary) to the point where there is some confusion about exactly what comprises the true _Black-Litterman_ model.\n",
    "\n",
    "I shall use a nomenclature that is consistent with \\cite{walters2011black}. Walters classifies implementations in two broad categories. The first category was implemented by \\cite{black1992global} and \\cite{he1999intuition}, and Walters refers to these as the _Reference Model_. The second category consists of well known implementations described in \\cite{satchell2000demystification} and a series of papers by Meucci (e.g. \\cite{meucci2005beyond}, \\cite{meucci2009enhancing}, \\cite{meucci2012fully}). In these models, the $\\tau$ parameter is eliminated, either by setting it to 1 or by incorporating it into the $\\Omega$ matrix.\n",
    "\n",
    "For the rest of this document, I shall be restricting myself to the _Reference Model_ as originally described in \\cite{black1992global} and \\cite{he1999intuition}, and I shall not be implementing the extensions of Meucci and others.\n",
    "\n",
    "### Implementation Overview\n",
    "\n",
    "The rest of this notebook proceeds as follows. In the following section, I shall implement the Black Litterman procedure in Python and annotate the code as I proceed, to illustrate each step. I then use the code to exactly reproduce the results in \\cite{he1999intuition}.\n",
    " \n",
    "Having established that the code accurately implements the Black Litterman procedure, I shall get down apply the procedure to the Fama French 6-portfolio allocation problem. Along the way, my tests will impose absolute views as well as relative views, and test the impact of the procedure on portfolios using a range of Seven different prediction strategies to obtain views. I also backtest these strategies over time and examine various portfolio metrics, while comparing the Black Litterman derived (BL) expected returns being supplied to an optimizer with weights obtained from Naive Mean-Variance optimization using expected returns and covariance matrixes directly from the prediction strategy. Finally, I conclude the section by examining the impact of these portfolios on transaction costs. \n",
    " \n",
    "## Annotated Implementation of Black-Litterman\n",
    "### The Code\n",
    "\n",
    "The Black Litterman procedure is implemented in Python in the function `bl`. Before we implement the body of `bl`, let's build a few helper functions that will hopefully make the code a bit easier to understand and deal with.\n",
    "\n",
    "numpy treats a column vector differently from a 1 dimensional array. In order to consistently use column vectors, the following helper function takes either a numpy array or a numpy one-column matrix (i.e. a column vector) and returns the data as a column vector. Let's call this function `as_colvec`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def as_colvec(x):\n",
    "    if (x.ndim == 2):\n",
    "        return x\n",
    "    else:\n",
    "        return np.expand_dims(x, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "as_colvec(np.arange(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the first step in the Black Litterman procedure was to reverse engineer the implied returns vector $\\pi$ from a set of portfolio weights $w$. \n",
    "\n",
    "$$\\pi = \\delta\\Sigma w$$\n",
    "\n",
    "This is performed by the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implied_returns(delta, sigma, w):\n",
    "    \"\"\"\n",
    "Obtain the implied expected returns by reverse engineering the weights\n",
    "Inputs:\n",
    "delta: Risk Aversion Coefficient (scalar)\n",
    "sigma: Variance-Covariance Matrix (N x N) as DataFrame\n",
    "    w: Portfolio weights (N x 1) as Series\n",
    "Returns an N x 1 vector of Returns as Series\n",
    "    \"\"\"\n",
    "    ir = delta * sigma.dot(w).squeeze() # to get a series from a 1-column dataframe\n",
    "    ir.name = 'Implied Returns'\n",
    "    return ir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we noted previously, \\cite{he1999intuition} suggest that if the investor does not have a specific way to explicitly quantify the uncertaintly associated with the view in the $\\Omega$ matrix, one could make the simplifying assumption that $\\Omega$ is proportional to the variance of the prior.\n",
    "\n",
    "Specifically, they suggest that:\n",
    "\n",
    "$$\\Omega = diag(P (\\tau \\Sigma) P^T) $$\n",
    "\n",
    "This is implemented in Python as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes that Omega is proportional to the variance of the prior\n",
    "def proportional_prior(sigma, tau, p):\n",
    "    \"\"\"\n",
    "    Returns the He-Litterman simplified Omega\n",
    "    Inputs:\n",
    "    sigma: N x N Covariance Matrix as DataFrame\n",
    "    tau: a scalar\n",
    "    p: a K x N DataFrame linking Q and Assets\n",
    "    returns a P x P DataFrame, a Matrix representing Prior Uncertainties\n",
    "    \"\"\"\n",
    "    helit_omega = p.dot(tau * sigma).dot(p.T)\n",
    "    # Make a diag matrix from the diag elements of Omega\n",
    "    return pd.DataFrame(np.diag(np.diag(helit_omega.values)),index=p.index, columns=p.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this function to compute the posterior expected returns as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "def bl(w_prior, sigma_prior, p, q,\n",
    "                omega=None,\n",
    "                delta=2.5, tau=.02):\n",
    "    \"\"\"\n",
    "# Computes the posterior expected returns based on \n",
    "# the original black litterman reference model\n",
    "#\n",
    "# W.prior must be an N x 1 vector of weights, a Series\n",
    "# Sigma.prior is an N x N covariance matrix, a DataFrame\n",
    "# P must be a K x N matrix linking Q and the Assets, a DataFrame\n",
    "# Q must be an K x 1 vector of views, a Series\n",
    "# Omega must be a K x K matrix a DataFrame, or None\n",
    "# if Omega is None, we assume it is\n",
    "#    proportional to variance of the prior\n",
    "# delta and tau are scalars\n",
    "    \"\"\"\n",
    "    if omega is None:\n",
    "        omega = proportional_prior(sigma_prior, tau, p)\n",
    "    # Force w.prior and Q to be column vectors\n",
    "    # How many assets do we have?\n",
    "    N = w_prior.shape[0]\n",
    "    # And how many views?\n",
    "    K = q.shape[0]\n",
    "    # First, reverse-engineer the weights to get pi\n",
    "    pi = implied_returns(delta, sigma_prior,  w_prior)\n",
    "    # Adjust (scale) Sigma by the uncertainty scaling factor\n",
    "    sigma_prior_scaled = tau * sigma_prior  \n",
    "    # posterior estimate of the mean, use the \"Master Formula\"\n",
    "    # we use the versions that do not require\n",
    "    # Omega to be inverted (see previous section)\n",
    "    # this is easier to read if we use '@' for matrixmult instead of .dot()\n",
    "    #     mu_bl = pi + sigma_prior_scaled @ p.T @ inv(p @ sigma_prior_scaled @ p.T + omega) @ (q - p @ pi)\n",
    "    mu_bl = pi + sigma_prior_scaled.dot(p.T).dot(inv(p.dot(sigma_prior_scaled).dot(p.T) + omega).dot(q - p.dot(pi).values))\n",
    "    # posterior estimate of uncertainty of mu.bl\n",
    "#     sigma_bl = sigma_prior + sigma_prior_scaled - sigma_prior_scaled @ p.T @ inv(p @ sigma_prior_scaled @ p.T + omega) @ p @ sigma_prior_scaled\n",
    "    sigma_bl = sigma_prior + sigma_prior_scaled - sigma_prior_scaled.dot(p.T).dot(inv(p.dot(sigma_prior_scaled).dot(p.T) + omega)).dot(p).dot(sigma_prior_scaled)\n",
    "    return (mu_bl, sigma_bl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Example: Absolute Views\n",
    "\n",
    "We start with a simple 2-Asset example. Let's start with an example from _Statistical Models and Methods for Financial Markets (Springer Texts in Statistics) 2008th Edition, Tze Lai and Haipeng Xing_.\n",
    "\n",
    "Consider the portfolio consisting of just two stocks: Intel (INTC) and Pfizer (PFE).\n",
    "\n",
    "From Table 3.1 on page 72 of the book, we obtain the covariance matrix (multipled by $10^4$)\n",
    "\n",
    "\\begin{array}{lcc}\n",
    "INTC & 46.0 & 1.06 \\\\\n",
    "PFE   & 1.06 & 5.33\n",
    "\\end{array}\n",
    "\n",
    "Assume that Intel has a market capitalization of approximately USD 80B and that of Pfizer is approximately USD 100B (this is not quite accurate, but works just fine as an example!).\n",
    "Thus, if you held a market-cap weighted portfolio you would hold INTC and PFE with the following weights: $W_{INTC} = 80/180 = 44\\%, W_{PFE} = 100/180 = 56\\%$. These appear to be reasonable weights without an extreme allocation to either stock, even though Pfizer is slightly overweighted.\n",
    "\n",
    "We can compute the equilibrium implied returns $\\pi$ as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTC    0.052084\n",
       "PFE     0.008628\n",
       "Name: Implied Returns, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers = ['INTC', 'PFE']\n",
    "s = pd.DataFrame([[46.0, 1.06], [1.06, 5.33]], index=tickers, columns=tickers) *  10E-4\n",
    "pi = implied_returns(delta=2.5, sigma=s, w=pd.Series([.44, .56], index=tickers))\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the equilibrium implied returns for INTC are a bit more than 5\\% and a bit less than 1\\% for PFE.\n",
    "\n",
    "Assume that the investor thinks that Intel will return 2\\% and that Pfizer is poised to rebounce, and will return 4\\% . We can now examine the optimal weights according to the Markowitz procedure.\n",
    "What would happen if we used these expected returns to compute the Optimal Max Sharpe Ratio portfolio?\n",
    "\n",
    "The Max Sharpe Ratio (MSR) Portfolio weights are easily computed in explicit form if there are no constraints on the weights.\n",
    "The weights are given by the expression (e.g. See  \\cite{campbell1996econometrics} page 188 Equation 5.2.28):\n",
    "\n",
    "$$ W_{MSR} = \\frac{\\Sigma^{-1}\\mu_e}{\\bf{1}^T \\Sigma^{-1}\\mu_e} $$\n",
    "\n",
    "where $\\mu_e$ is the vector of expected excess returns and $\\Sigma$ is the variance-covariance matrix.\n",
    "\n",
    "This is implemented as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for convenience and readability, define the inverse of a dataframe\n",
    "def inverse(d):\n",
    "    \"\"\"\n",
    "    Invert the dataframe by inverting the underlying matrix\n",
    "    \"\"\"\n",
    "    return pd.DataFrame(inv(d.values), index=d.columns, columns=d.index)\n",
    "\n",
    "def w_msr(sigma, mu, scale=True):\n",
    "    \"\"\"\n",
    "    Optimal (Tangent/Max Sharpe Ratio) Portfolio weights\n",
    "    by using the Markowitz Optimization Procedure\n",
    "    Mu is the vector of Excess expected Returns\n",
    "    Sigma must be an N x N matrix as a DataFrame and Mu a column vector as a Series\n",
    "    This implements page 188 Equation 5.2.28 of\n",
    "    \"The econometrics of financial markets\" Campbell, Lo and Mackinlay.\n",
    "    \"\"\"\n",
    "    w = inverse(sigma).dot(mu)\n",
    "    if scale:\n",
    "        w = w/sum(w) # fix: this assumes all w is +ve\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the investor expects that Intel will return 2\\% and Pfizer will return 4\\% . We can now examine the optimal weights obtained by naively implementing the Markowitz procedure with these expected returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTC     3.41\n",
       "PFE     96.59\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_exp = pd.Series([.02, .04],index=tickers) # INTC and PFE\n",
    "np.round(w_msr(s, mu_exp)*100, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consistent with the poor reputation of naive Markowitz optimization, the Markwitz procedure places an unrealistic weight of more than 96\\% in Pfizer and less than 4\\% in Intel. This is completely impractical and no reasonable investor would make such dramatic bets.\n",
    "\n",
    "In contrast, let us now find the weights that the Black Litterman procedure would place. We allow $\\Omega$ to be computed automatically, and are willing to use all the other defaults. We find the Black Litterman weights as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTC    0.037622\n",
       "PFE     0.024111\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Absolute view 1: INTC will return 2%\n",
    "# Absolute view 2: PFE will return 4%\n",
    "q = pd.Series({'INTC': 0.02, 'PFE': 0.04})\n",
    "\n",
    "# The Pick Matrix\n",
    "# For View 2, it is for PFE\n",
    "p = pd.DataFrame([\n",
    "# For View 1, this is for INTC\n",
    "    {'INTC': 1, 'PFE': 0},\n",
    "# For View 2, it is for PFE\n",
    "    {'INTC': 0, 'PFE': 1}\n",
    "    ])\n",
    "\n",
    "# Find the Black Litterman Expected Returns\n",
    "bl_mu, bl_sigma = bl(w_prior=pd.Series({'INTC':.44, 'PFE':.56}), sigma_prior=s, p=p, q=q)\n",
    "# Black Litterman Implied Mu\n",
    "bl_mu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior returns returned by the procedure are clearly weighted between that of the equilibrium implied expected returns (in the range of 5\\% and 1\\%) and that of the investor (2\\% and 4\\%). The question is are these weights likely to yield more realistic portfolios? To answer that question we supply the Black Litterman expected returns and covariance matrix to the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTC    0.140692\n",
       "PFE     0.859308\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the Black Litterman expected returns to get the Optimal Markowitz weights\n",
    "w_msr(bl_sigma, bl_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we get much more reasonable weights than we did with naive optimization. These weights are also much closer to the 45-55 mix in the cap weighted portfolio.\n",
    "On the other hand, they respect the investor's view that expects Pfizer to rebound, and places a higher weight on Pfizer relative to the cap weighted portfolio.\n",
    "\n",
    "### A Simple Example: Relative Views\n",
    "\n",
    "In this example, we examine relative views. We stick with our simple 2-stock example. Recall that the Cap-Weighted implied expected returns are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTC    0.052084\n",
       "PFE     0.008628\n",
       "Name: Implied Returns, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expected returns inferred from the cap-weights\n",
    "pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall also that the cap-weighted portfolio is approximately a 45-55 mix of Intel and Pfizer.\n",
    "\n",
    "Assume instead that the investor feels that the Intel will outperform Pfizer by only 2\\%. This view is implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTC    0.041374\n",
       "PFE     0.009646\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = pd.Series([\n",
    "# Relative View 1: INTC will outperform PFE by 2%\n",
    "  0.02\n",
    "    ]\n",
    ")\n",
    "# The Pick Matrix\n",
    "p = pd.DataFrame([\n",
    "  # For View 1, this is for INTC outperforming PFE\n",
    "  {'INTC': +1, 'PFE': -1}\n",
    "])\n",
    "\n",
    "# Find the Black Litterman Expected Returns\n",
    "bl_mu, bl_sigma = bl(w_prior=pd.Series({'INTC': .44, 'PFE': .56}), sigma_prior=s, p=p, q=q)\n",
    "# Black Litterman Implied Mu\n",
    "bl_mu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we see that the Black Litterman expected returns are a blend between the cap-weight implied weights and the investor view. The outperformance of Intel in the implied returns is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043456"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi[0]-pi[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, the investor felt it only would be 2\\%. The expected returns returned by the Black Litterman procedure show a spread that is a blend between the cap-weight implied returns and that of the investor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031728"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl_mu[0]-bl_mu[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, the weights in the Optimized portfolio when we use these expected returns are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTC    0.347223\n",
       "PFE     0.652777\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the Black Litterman expected returns and covariance matrix\n",
    "w_msr(bl_sigma, bl_mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seem like reasonable weights, and demonstrates the power of using the Black Litterman procedure. In contrast, consider the weights we would get if we implemented the same view without Black Litterman. We set the returns of Intel and Pfizer to be 3\\% and 1\\% respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTC    0.258528\n",
       "PFE     0.741472\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_msr(s, [.03, .01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights are significantly more dramatic than one might be willing to implement, and are likely unwarranted given the relatively weak view. In fact, if the same view were implemented as Intel and Pfizer returning 2\\% and 0\\%, the results are even more extreme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INTC    1.248244\n",
       "PFE    -0.248244\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_msr(s, [.02, .0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the Markowitz recommends shorting Pfizer to the extent of nearly 25\\% of the portfolio and leveraging Intel to 125\\%. Clearly this is not a plausible allocation based on the simple view expressed above.\n",
    "\n",
    "## Reproducing the He-Litterman (1999) Results\n",
    "\n",
    "We now reproduce the results in the He-Litterman paper that first detailed the steps in the procedure. We obtained the data by typing it in from the He-Litterman tables, and used it to test the implementation.\n",
    "\n",
    "The He-Litterman example involves an international allocation between 7 countries. The data is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU    3.9\n",
       "CA    6.9\n",
       "FR    8.4\n",
       "DE    9.0\n",
       "JP    4.3\n",
       "UK    6.8\n",
       "US    7.6\n",
       "Name: Implied Returns, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 7 countries ...\n",
    "countries  = ['AU', 'CA', 'FR', 'DE', 'JP', 'UK', 'US'] \n",
    "# Table 1 of the He-Litterman paper\n",
    "# Correlation Matrix\n",
    "rho = pd.DataFrame([\n",
    "    [1.000,0.488,0.478,0.515,0.439,0.512,0.491],\n",
    "    [0.488,1.000,0.664,0.655,0.310,0.608,0.779],\n",
    "    [0.478,0.664,1.000,0.861,0.355,0.783,0.668],\n",
    "    [0.515,0.655,0.861,1.000,0.354,0.777,0.653],\n",
    "    [0.439,0.310,0.355,0.354,1.000,0.405,0.306],\n",
    "    [0.512,0.608,0.783,0.777,0.405,1.000,0.652],\n",
    "    [0.491,0.779,0.668,0.653,0.306,0.652,1.000]\n",
    "], index=countries, columns=countries)\n",
    "\n",
    "# Table 2 of the He-Litterman paper: volatilities\n",
    "vols = pd.DataFrame([0.160,0.203,0.248,0.271,0.210,0.200,0.187],index=countries, columns=[\"vol\"]) \n",
    "# Table 2 of the He-Litterman paper: cap-weights\n",
    "w_eq = pd.DataFrame([0.016,0.022,0.052,0.055,0.116,0.124,0.615], index=countries, columns=[\"CapWeight\"])\n",
    "# Compute the Covariance Matrix\n",
    "sigma_prior = vols.dot(vols.T) * rho\n",
    "# Compute Pi and compare:\n",
    "pi = implied_returns(delta=2.5, sigma=sigma_prior, w=w_eq)\n",
    "(pi*100).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of $\\pi$ computed by the Python code exactly matches column 3 of Table 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View 1: Germany vs Rest of Europe\n",
    "\n",
    "Next, we impose the view that German equities will outperform the rest of European equities by 5\\%.\n",
    "\n",
    "The other European equities are France and the UK. We split the outperformance proportional to the Market Caps of France and the UK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AU</th>\n",
       "      <th>CA</th>\n",
       "      <th>FR</th>\n",
       "      <th>DE</th>\n",
       "      <th>JP</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-29.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-70.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AU   CA    FR     DE   JP    UK   US\n",
       "0  0.0  0.0 -29.5  100.0  0.0 -70.5  0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Germany will outperform other European Equities (i.e. FR and UK) by 5%\n",
    "q = pd.Series([.05]) # just one view\n",
    "# start with a single view, all zeros and overwrite the specific view\n",
    "p = pd.DataFrame([0.]*len(countries), index=countries).T\n",
    "# find the relative market caps of FR and UK to split the\n",
    "# relative outperformance of DE ...\n",
    "w_fr =  w_eq.loc[\"FR\"]/(w_eq.loc[\"FR\"]+w_eq.loc[\"UK\"])\n",
    "w_uk =  w_eq.loc[\"UK\"]/(w_eq.loc[\"FR\"]+w_eq.loc[\"UK\"])\n",
    "p.iloc[0]['DE'] = 1.\n",
    "p.iloc[0]['FR'] = -w_fr\n",
    "p.iloc[0]['UK'] = -w_uk\n",
    "(p*100).round(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The results of implementing this view appear in the He-Litterman paper in Table 4. This exactly reproduces column 1 of Table 4. Next, we examine the values of $\\mu^{BL}$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     4.3\n",
       "CA     7.6\n",
       "FR     9.3\n",
       "DE    11.0\n",
       "JP     4.5\n",
       "UK     7.0\n",
       "US     8.1\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = 2.5\n",
    "tau = 0.05 # from Footnote 8\n",
    "# Find the Black Litterman Expected Returns\n",
    "bl_mu, bl_sigma = bl(w_eq, sigma_prior, p, q, tau = tau)\n",
    "(bl_mu*100).round(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  Black Litterman expected returns computed by the code exactly reproduces column 2 of Table 4.\n",
    "\n",
    "He-Litterman compute the optimal portfolio $w^*$ as follows (this is Equation (13) on page 6 of their paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     1.5\n",
       "CA     2.1\n",
       "FR    -4.0\n",
       "DE    35.4\n",
       "JP    11.0\n",
       "UK    -9.5\n",
       "US    58.6\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def w_star(delta, sigma, mu):\n",
    "    return (inverse(sigma).dot(mu))/delta\n",
    "\n",
    "wstar = w_star(delta=2.5, sigma=bl_sigma, mu=bl_mu)\n",
    "# display w*\n",
    "(wstar*100).round(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computed $w^*$ exactly replicates column 3 ($w^*$) of Table 4. Finally, they compute $w^* - \\frac{w_{eq}}{1+\\tau}$ which is the difference in weights between the optimal portfolio and the equilibrium portfolio (they use unscaled weights) in column 4. We replicate that column as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     0.0\n",
       "CA    -0.0\n",
       "FR    -8.9\n",
       "DE    30.2\n",
       "JP     0.0\n",
       "UK   -21.3\n",
       "US     0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_eq  = w_msr(delta*sigma_prior, pi, scale=False)\n",
    "# Display the difference in Posterior and Prior weights\n",
    "np.round(wstar - w_eq/(1+tau), 3)*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which exactly matches Column 4 of Table 4. This completes our reproduction of the first view in He-Litterman (1999).\n",
    "\n",
    "Note that this demonstrates the power of the approach. The weights for assets that do not involve the view remain unchanged. The two underperforming countries (according to the view) are underweighted, while the overperforming country is overweighted, but not to the extreme extent that a naive portfolio optimizer would have produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View 2: Canada vs US\n",
    "\n",
    "For their second case, He and Litterman implement the additional view that Canadian Equities will outperform US Equities by 3\\%. The results are in (their) Table 5, which we shall now reproduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AU</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FR</th>\n",
       "      <td>-29.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DE</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JP</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK</th>\n",
       "      <td>-70.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1\n",
       "AU    0.0    0.0\n",
       "CA    0.0  100.0\n",
       "FR  -29.5    0.0\n",
       "DE  100.0    0.0\n",
       "JP    0.0    0.0\n",
       "UK  -70.5    0.0\n",
       "US    0.0 -100.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view2 = pd.Series([.03], index=[1])\n",
    "q = q.append(view2)\n",
    "pick2 = pd.DataFrame([0.]*len(countries), index=countries, columns=[1]).T\n",
    "p = p.append(pick2)\n",
    "p.iloc[1]['CA']=+1\n",
    "p.iloc[1]['US']=-1\n",
    "np.round(p.T, 3)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches columns 1 and 2 of Table 5. We now compute the Black Litterman weights as\n",
    "before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     4.4\n",
       "CA     8.7\n",
       "FR     9.5\n",
       "DE    11.2\n",
       "JP     4.6\n",
       "UK     7.0\n",
       "US     7.5\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl_mu, bl_sigma = bl(w_eq, sigma_prior, p, q, tau = tau)\n",
    "np.round(bl_mu*100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Black Litterman expected returns computed by the Python code exactly reproduces column 3 of\n",
    "Table 5.\n",
    "He-Litterman compute the optimal portfolio w âˆ— as follows (this is Equation (13) on page 6 of\n",
    "their paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     1.5\n",
       "CA    41.9\n",
       "FR    -3.4\n",
       "DE    33.6\n",
       "JP    11.0\n",
       "UK    -8.2\n",
       "US    18.8\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wstar = w_star(delta=2.5, sigma=bl_sigma, mu=bl_mu)\n",
    "# display w*\n",
    "(wstar*100).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computed $w^*$ exactly replicates column 4 ($w^*$) of Table 5. Finally, as in the previous case, they compute $w^* - \\frac{w_{eq}}{1+\\tau}$ in column 5. We replicate that column as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     0.0\n",
       "CA    39.8\n",
       "FR    -8.4\n",
       "DE    28.4\n",
       "JP    -0.0\n",
       "UK   -20.0\n",
       "US   -39.8\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_eq  = w_msr(delta*sigma_prior, pi, scale=False)\n",
    "# Display the difference in Posterior and Prior weights\n",
    "np.round(wstar - w_eq/(1+tau), 3)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which exactly reproduces the last column of Table 5 of their paper.\n",
    "\n",
    "Once again, we see the power of the approach. The weights for assets that do not involve the view (AU, JP) remain unchanged. The two underperforming countries (FR, UK, US, according to the view) are underweighted, while the overperforming countries (CA, DE) are overweighted, but not to the extreme extent that a naive portfolio optimizer would have produced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View 3: More Bullish Canada vs US\n",
    "\n",
    "For their third case, He and Litterman alter the second view that Canadian Equities will outperform US Equities by increasing the expected out-performance from the previously stated 3\\% to 4\\%. The results are in Table 6 of their paper, which we shall now reproduce.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.05\n",
       "1    0.04\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[1] = .04\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that P remains unchanged since we have only altered Q, not P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AU</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FR</th>\n",
       "      <td>-29.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DE</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JP</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK</th>\n",
       "      <td>-70.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1\n",
       "AU    0.0    0.0\n",
       "CA    0.0  100.0\n",
       "FR  -29.5    0.0\n",
       "DE  100.0    0.0\n",
       "JP    0.0    0.0\n",
       "UK  -70.5    0.0\n",
       "US    0.0 -100.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(p.T*100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches columns 1 and 2 of Table 6. We now compute the Black Litterman weights as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     4.4\n",
       "CA     9.1\n",
       "FR     9.5\n",
       "DE    11.3\n",
       "JP     4.6\n",
       "UK     7.0\n",
       "US     7.3\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl_mu, bl_sigma = bl(w_eq, sigma_prior, p, q, tau = tau)\n",
    "np.round(bl_mu, 3)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  Black Litterman expected returns computed by my code exactly reproduces column 3 of Table 6.\n",
    "\n",
    "He-Litterman compute the optimal portfolio $w^*$ as follows (this is Equation (13) on page 6 of their paper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     1.5\n",
       "CA    53.3\n",
       "FR    -3.3\n",
       "DE    33.1\n",
       "JP    11.0\n",
       "UK    -7.8\n",
       "US     7.3\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wstar = w_star(delta=2.5, sigma=bl_sigma, mu=bl_mu)\n",
    "# display w*\n",
    "(wstar*100).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computed $w^*$ exactly replicates column 4 ($w^*$) of Table 7. Finally, as in the previous case, they compute $w^* - \\frac{w_{eq}}{1+\\tau}$ in column 6. We replicate that column as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     0.0\n",
       "CA    51.3\n",
       "FR    -8.2\n",
       "DE    27.8\n",
       "JP    -0.0\n",
       "UK   -19.6\n",
       "US   -51.3\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_eq  = w_msr(delta*sigma_prior, pi, scale=False)\n",
    "# Display the difference in Posterior and Prior weights\n",
    "np.round(wstar - w_eq/(1+tau), 3)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which exactly reproduces the last column of Table 6 of their paper. Again, we see how the weights increase allocations consistent with the view, but keep allocations from getting extreme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View 4: Increasing View Uncertainty\n",
    "\n",
    "As a final step, He and Litterman demonstrate the effect of $\\Omega$. They increase the uncertainty associated with the first of the two views (i.e. the one that Germany will outperform the rest of Europe). First we compute the default value of $\\Omega$ and then increase the uncertainty associated with the first view alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AU</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CA</th>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FR</th>\n",
       "      <td>-29.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DE</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JP</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UK</th>\n",
       "      <td>-70.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1\n",
       "AU    0.0    0.0\n",
       "CA    0.0  100.0\n",
       "FR  -29.5    0.0\n",
       "DE  100.0    0.0\n",
       "JP    0.0    0.0\n",
       "UK  -70.5    0.0\n",
       "US    0.0 -100.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the default \"Proportional to Prior\" assumption\n",
    "omega = proportional_prior(sigma_prior, tau, p)\n",
    "# Now, double the uncertainty associated with View 1\n",
    "omega.iloc[0,0] = 2*omega.iloc[0,0]\n",
    "np.round(p.T*100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches columns 1 and 2 of Table 7 (which is if course, unchanged, since we have only altered $\\Omega$, not Q or P). We now compute the Black Litterman weights as before, but supplying the value of $\\Omega$ we just adjusted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     4.3\n",
       "CA     8.9\n",
       "FR     9.3\n",
       "DE    10.6\n",
       "JP     4.6\n",
       "UK     6.9\n",
       "US     7.2\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl_mu, bl_sigma = bl(w_eq, sigma_prior, p, q, tau = tau, omega=omega)\n",
    "np.round(bl_mu, 3)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  Black Litterman expected returns computed by the code exactly reproduces column 3 of Table 7.\n",
    "\n",
    "He-Litterman compute the optimal portfolio $w^*$ as follows (this is Equation (13) on page 6 of their paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU     1.5\n",
       "CA    53.9\n",
       "FR    -0.5\n",
       "DE    23.6\n",
       "JP    11.0\n",
       "UK    -1.1\n",
       "US     6.8\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wstar = w_star(delta=2.5, sigma=bl_sigma, mu=bl_mu)\n",
    "# display w*\n",
    "(wstar*100).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computed $w^*$ exactly replicates column 4 ($w^*$) of Table 7. Finally, as in the previous case, they compute $w^* - \\frac{w_{eq}}{1+\\tau}$ in column 6. We replicate that column as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AU    -0.0\n",
       "CA    51.8\n",
       "FR    -5.4\n",
       "DE    18.4\n",
       "JP     0.0\n",
       "UK   -13.0\n",
       "US   -51.8\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_eq  = w_msr(delta*sigma_prior, pi, scale=False)\n",
    "# Display the difference in Posterior and Prior weights\n",
    "np.round(wstar - w_eq/(1+tau), 3)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which exactly reproduces the last column of Table 7 of their paper. Again, we see how the weights increase allocations consistent with the view, but keep allocations from getting extreme.\n",
    "\n",
    "That concludes our reproduction of the paper. Note that He and Litterman also produce an extra table (Table 8) which demonstrates the value of adding a third view. However, the third view is identical to the values implied by the equilibrium and as a result, they produce exactly the same results as Table 7. I do not bother reproduce it here since the results are exactly the same as Table 7.\n",
    "\n",
    "## Try it on Industry Data ...\n",
    "\n",
    "Now that you've reproduced the results, you should be able to run the code on the Fama-French Industry Portfolios ...\n",
    "\n",
    "Start out by loading the data as follows, and then play around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edhec_risk_kit_206 as erk\n",
    "\n",
    "ind49_rets = erk.get_ind_returns(weighting=\"vw\", n_inds=49)[\"2014\":]\n",
    "ind49_mcap = erk.get_ind_market_caps(49, weights=True)[\"2014\":]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
